{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_assets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPIJGBbOOKhrlic7Ntu7RH6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zey-o/Engineering_OpenSea_LostPoets/blob/main/5_assets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "SmZHKlQfwZSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c8ffedc-4fd0-4c8f-877f-033b000c9872"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook first loads the origin images as scraped from the URL's provided on the the OpenSea website. It then adapts  TensorFlow's DCGAN's tutorial in order to produce 25 new poet images. The new poet images are then used in the game application. \n",
        "\n",
        "In order of operation: \n",
        "1. The NB first loads the images \n",
        "2. Makes generator/discriminator models \n",
        "3. Defines loss functions & optimizers \n",
        "4. Trains the model "
      ],
      "metadata": {
        "id": "WFsRx6NydQHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import necessary & image libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from glob import glob\n",
        "import os \n",
        "import PIL\n",
        "import cv2\n",
        "\n",
        "import time\n",
        "from IPython import display "
      ],
      "metadata": {
        "id": "6_F23xiRwu2C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8nSlNvppwHdV"
      },
      "outputs": [],
      "source": [
        "# import libraries for modeling \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import (Dense, \n",
        "                                     BatchNormalization, \n",
        "                                     LeakyReLU, \n",
        "                                     Reshape, \n",
        "                                     Conv2DTranspose,\n",
        "                                     Conv2D,\n",
        "                                     Dropout,\n",
        "                                     Flatten)\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & reshape images"
      ],
      "metadata": {
        "id": "2nA_93CHKP7l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load images and then resize them to the right input size that will be put into the GAN models."
      ],
      "metadata": {
        "id": "Or5Fg61_Kihb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = pathlib.Path(\"drive/MyDrive/LostPoets/origin_images\")"
      ],
      "metadata": {
        "id": "GgfpUxsGwYXJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_count = len(list(data_dir.glob('**/*.png')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAXQCkqy1PJ2",
        "outputId": "8ee12586-f1c1-45c4-8955-1e56a18e7ad2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "402\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = PIL.Image.open(os.path.join(folder,filename))\n",
        "        img1 = img.convert('L')\n",
        "        img2 = np.expand_dims(img1, axis=-1)\n",
        "        if img2 is not None:\n",
        "            images.append(img2)\n",
        "    return images\n"
      ],
      "metadata": {
        "id": "pF6JAP3H1y1d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = load_images_from_folder(data_dir)"
      ],
      "metadata": {
        "id": "SxsRvpsS14qI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check out the size of a single image\n",
        "train_images[0].size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzQ0XYx4Fyo-",
        "outputId": "3c708462-2029-43be-e6f7-dd51aabd5c21"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "262144"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images = np.array(train_images)\n",
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWj_PImR2l0H",
        "outputId": "11f3d22c-b2da-4e63-abef-610ddc4768a8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(402, 512, 512, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resize them so that they are 28x28\n",
        "train_images = tf.image.resize(train_images, [28, 28])\n",
        "train_images = train_images.numpy()\n",
        "train_images.shape"
      ],
      "metadata": {
        "id": "Q5vTJyCLB0Id",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171c5902-9b7b-4feb-ce4e-9d9d9fdbcf91"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(402, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize images to [-1, 1]\n",
        "train_images = (train_images - 127.5) / 127.5"
      ],
      "metadata": {
        "id": "_V434spE-PNu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 28"
      ],
      "metadata": {
        "id": "05Ng1eg43LxZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "V9mU_f3i2ltx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the Generator / Discriminator Models"
      ],
      "metadata": {
        "id": "ul1__x09LJx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(7*7*256, use_bias=False, input_shape=(10000,)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256) # Note: None is the batch size\n",
        "\n",
        "    model.add(Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LeakyReLU())\n",
        "\n",
        "    model.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "4lFwk1AI2loB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = make_generator_model()"
      ],
      "metadata": {
        "id": "TgE4IGv-2lk4"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noise = tf.random.normal([1, 10000])\n",
        "generated_image = generator(noise, training=False)\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "metadata": {
        "id": "egZ6-ib_5pRU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "09e41deb-bcdc-49d2-98e8-12b607e00def"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3ad01d4250>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    \n",
        "    model.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "_TNIS8hC5o6U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "discriminator = make_discriminator_model()"
      ],
      "metadata": {
        "id": "mfqThndd5o2o"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chIWMzdF5ozR",
        "outputId": "732df4dc-03fb-4526-a935-368b95518064"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[-0.00221204]], shape=(1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Functions & Optimizers"
      ],
      "metadata": {
        "id": "ibNmPM7TLqGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "id": "0OXmLwjw5owL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for saving checkpoints since there will be quite a few epochs \n",
        "checkpoint_dir = 'drive/MyDrive/LostPoets/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "metadata": {
        "id": "Vgqn0aGQ5osw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training "
      ],
      "metadata": {
        "id": "2zY_3pRONUtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples_to_generate = 24\n",
        "noise_dim = 10000\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "metadata": {
        "id": "Mk4sL8a35opt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  \n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, \n",
        "                                               generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, \n",
        "                                                discriminator.trainable_variables)\n",
        "    \n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "metadata": {
        "id": "z3Dq1Ij-2lbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1000000"
      ],
      "metadata": {
        "id": "mSwgeA4i7z8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, epochs):\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    \n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    if (epoch +1) % 10000 ==0: \n",
        "      start = time.time()\n",
        "      generate_and_save_images(generator, \n",
        "                               epoch+1, \n",
        "                               seed)\n",
        "      print('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "      # checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "      \n",
        "      \n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)\n"
      ],
      "metadata": {
        "id": "xu3a5cJF8JQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "\n",
        "  predictions = model(test_input, training=False)\n",
        "  \n",
        "  images_dir = \"drive/MyDrive/LostPoets/generated_images_abc\"\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "      plt.savefig(f'{images_dir}/{i}.png', dpi = 200)\n"
      ],
      "metadata": {
        "id": "oKGcaTSljvL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "metadata": {
        "id": "9FxkJcX6XONt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bafa52c0-ec6a-4768-e53c-4dd85cc84d87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time for epoch 10000 is 9.82117247581482 sec\n",
            "Time for epoch 20000 is 19.82572293281555 sec\n",
            "Time for epoch 30000 is 35.043621301651 sec\n",
            "Time for epoch 40000 is 47.76874876022339 sec\n",
            "Time for epoch 50000 is 59.50942873954773 sec\n",
            "Time for epoch 60000 is 72.79707050323486 sec\n",
            "Time for epoch 70000 is 89.09254336357117 sec\n",
            "Time for epoch 80000 is 88.24789547920227 sec\n",
            "Time for epoch 90000 is 98.116943359375 sec\n",
            "Time for epoch 100000 is 109.62735748291016 sec\n",
            "Time for epoch 110000 is 134.8569097518921 sec\n",
            "Time for epoch 120000 is 147.5819594860077 sec\n",
            "Time for epoch 130000 is 162.15283012390137 sec\n",
            "Time for epoch 140000 is 180.89732241630554 sec\n",
            "Time for epoch 150000 is 193.94235253334045 sec\n",
            "Time for epoch 160000 is 202.99684834480286 sec\n",
            "Time for epoch 170000 is 177.44988179206848 sec\n",
            "Time for epoch 180000 is 193.65848183631897 sec\n",
            "Time for epoch 190000 is 206.0128321647644 sec\n",
            "Time for epoch 200000 is 208.7032253742218 sec\n",
            "Time for epoch 210000 is 220.47586727142334 sec\n",
            "Time for epoch 220000 is 229.58279657363892 sec\n",
            "Time for epoch 230000 is 253.00633478164673 sec\n",
            "Time for epoch 240000 is 271.3346314430237 sec\n",
            "Time for epoch 250000 is 308.1279618740082 sec\n",
            "Time for epoch 260000 is 325.22304821014404 sec\n",
            "Time for epoch 270000 is 339.5024800300598 sec\n",
            "Time for epoch 280000 is 352.3096835613251 sec\n",
            "Time for epoch 290000 is 350.32137799263 sec\n",
            "Time for epoch 300000 is 377.60914182662964 sec\n",
            "Time for epoch 310000 is 337.2112412452698 sec\n",
            "Time for epoch 320000 is 361.48440742492676 sec\n",
            "Time for epoch 330000 is 414.3000900745392 sec\n",
            "Time for epoch 340000 is 427.39358282089233 sec\n",
            "Time for epoch 350000 is 365.5341033935547 sec\n",
            "Time for epoch 360000 is 392.31337118148804 sec\n",
            "Time for epoch 370000 is 448.42327189445496 sec\n",
            "Time for epoch 380000 is 475.0327744483948 sec\n",
            "Time for epoch 390000 is 488.39628410339355 sec\n",
            "Time for epoch 400000 is 487.68127274513245 sec\n",
            "Time for epoch 410000 is 526.0051295757294 sec\n",
            "Time for epoch 420000 is 543.7019777297974 sec\n",
            "Time for epoch 430000 is 551.3033699989319 sec\n",
            "Time for epoch 440000 is 460.75308418273926 sec\n",
            "Time for epoch 450000 is 497.4631042480469 sec\n",
            "Time for epoch 460000 is 541.1893579959869 sec\n",
            "Time for epoch 470000 is 584.2207317352295 sec\n",
            "Time for epoch 480000 is 610.5743067264557 sec\n",
            "Time for epoch 490000 is 611.2417047023773 sec\n",
            "Time for epoch 500000 is 613.1482982635498 sec\n",
            "Time for epoch 510000 is 625.2218906879425 sec\n",
            "Time for epoch 520000 is 564.5478177070618 sec\n"
          ]
        }
      ]
    }
  ]
}